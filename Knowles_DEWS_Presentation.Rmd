---
title: "Of Needles and Haystacks: Building a Statewide Dropout Early Warning System"
author: "Jared E. Knowles"
date: "September 28, 2015"
output:
  ioslides_presentation:
    css: custom.css
    widescreen: true
    self contained: true
    fig_width: 8
    fig_height: 8
---

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(dev="png", 
               echo=FALSE, warning=FALSE, message=FALSE, dpi=68)
```

## The Problem in Predicting School Dropout

1. Student dropout is a phenomenon that has roots much earlier in the education 
pipeline than high school. 

2. Current early identification strategies are of unknown accuracy, difficult 
to implement, and rely on late breaking data. 

3. The predictors of dropout vary across jurisdictions, making one size fits 
all models inefficient, biased, or both. 

4. Teachers, counselors, principals, and other school staff are spending valuable 
time assessing student dropout likelihood using manual identification. 

**State level early warning systems can help with this**

## Outline

- Current state of early warning models
- A gentle introduction to machine learning
- Deploying machine learning models to educators
- Future challenges


## The What of Early Warning Systems

- EWSs are common in many industries and have a number of other names -- predictive 
analytics, risk models, or machine learning
- Schools currently do a lot of work around identifying students-at-risk in 
response to federal and state laws and definitions
- EWS has traditionally been thought of as a high school tool, but is increasingly 
being introduced into middle school and earlier (Balfanz and Herzog 2006)
- Existing EWS models fall into three broad categories -- checklist, regression, 
and mixture/latent variable models

## What does the research say? Checklist models

- Checklist models are easy to implement, have been shown to have high out-of-sample 
validity in some school settings (Heppen and Therriault, 2008; Kennelly and
Monrad, 2007; Easton and Allensworth, 2005 & 2007; Roderick and Camburn, 1999; 
Allensworth, 2013, Balfanz and Herzog, 2006; Balfanz and Iver, 2007).
- The downside of such checklist based systems is that they tend to overemphasize individual
attributes like grades or attendance at the risk of oversimplifying the mechanisms underlying
students’ risk of dropping out (Gleason and Dynarski, 2002). 
- Often, educators are not given a strong sense of how well such indicator systems 
perform and, thus, may not find it easy to weigh the results of such a checklist 
against other evidence available to them.
- Hard to validate locally

## What does the research say? Regression models

- When using multiple indicators, regression techniques can provide a way to 
combine estimates and to validate indicators.
- Carl et al. (2013) use regression models to estimate a student’s probability of
high school completion conditional on the number and type of credits earned in the freshman
year.
- Regression models have been shown to be flexible to multiple school contexts, 
but have higher data requirements -- weighting of different indicators hard to 
implement on the fly.

## What does the research say? Mixture/Latent Variable Models

- Growth mixture models of math achievement growth are highly accurate in predicting 
dropout (Muthén 2004).
- Bowers and Sprott (2012) found that even when limiting the GMM to data 
available to most school staff, the model provides highly accurate predictions 
of individual students’ high school completion.
- Mixture/latent variable models are highly accurate, have demanding data requirements, 
and have not been proven out-of-sample (Muthén 2004)

## Choosing an EWS Statewide

**What do we want from an EWS?**

- **Early** identification in time to make interventions (grades5-8)
- **Accurately** identifying students who need assistance and those who do not
- **Transparency** in how predictions were made and how students are labeled
- **Reproducibility** in the predictions so they vary with changes in underlying behaviors 
not the models
- **Scaleable** to a diverse array of student and school contexts

## What does accuracy mean?

- Accuracy in binary classification is inherently a tradeoff between different 
types of error
- Accuracy of classification metrics must incorporate the trade-off between 
different types of misclassification

## Confusion Matrix: Performance Matrix Choices

<table style="float:center">
<tr>
<td colspan="2" rowspan="2"></td>
<td colspan="2" style="background-color:#1b85b8">Actual</td>			
</tr>
<tr>
<td>Non-grad</td>
<td>Graduate</td>
</tr>
<tr>
<td rowspan="2" style="background-color:#ae5a41">Predicted</td>
<td>Non-grad</td>
<td><b>a</b></td>
<td><b>b</b></td>
</tr>
<tr>
<td>Graduate</td>
<td><b>c</b></td>
<td><b>d</b></td>
</tr>
</table>

- Accuracy: $\frac{(a+d)}{(a+b+c+d)}$
- Precision (positive predictive value) = $\frac{a}{(a+b)}$
- Sensitivity (recall) = $\frac{a}{(a+c)}$
- Specificity (negative predictive value) = $\frac{d}{(b+d)}$
- False alarm (1-specificity) = $\frac{b}{(b+d)}$

## Confusion Matrix: Accuracy

<table>
<tr>
<td colspan="2" rowspan="2"></td>
<td colspan="2" style="background-color:#1b85b8; border: 2px solid">Actual</td>			
</tr>
<tr>
<td>Non-grad</td>
<td>Graduate</td>
</tr>
<tr>
<td rowspan="2" style="background-color:#ae5a41; border: 2px solid">Predicted</td>
<td>Non-grad</td>
<td  style="background-color:#c3cb71; border: 2px solid"><b>a</b></td>
<td><b>b</b></td>
</tr>
<tr>
<td>Graduate</td>
<td><b>c</b></td>
<td style="background-color:#c3cb71; border: 2px solid"><b>d</b></td>
</tr>
</table>

- Accuracy: $\frac{(a+d)}{(a+b+c+d)}$
- Accuracy is a good measure if our classes are fairly balanced and we care about 
overall correctly dividing the data into the groups. 
- If one group is much larger than another though, this method can be misleading.

## Confusion Matrix: Precision

<table>
<tr>
<td colspan="2" rowspan="2"></td>
<td colspan="2" style="background-color:#1b85b8; border: 2px solid">Actual</td>			
</tr>
<tr>
<td>Non-grad</td>
<td>Graduate</td>
</tr>
<tr>
<td rowspan="2" style="background-color:#ae5a41; border: 2px solid">Predicted</td>
<td>Non-grad</td>
<td  style="background-color:#c3cb71; border: 2px solid"><b>a</b></td>
<td  style="background-color:#c3cb71; border: 2px solid"><b>b</b></td>
</tr>
<tr>
<td>Graduate</td>
<td><b>c</b></td>
<td><b>d</b></td>
</tr>
</table>

- Precision (negative predictive value) = $\frac{a}{(a+b)}$
- Of all the cases we predict to be **non-graduates**, what proportion actually 
fail to graduate?
- If we are interested in the rarer **non-graduate** class, then this is a very useful metric 
to understand how good we are at identifying this group. 

## Confusion Matrix: Sensitivity

<table>
<tr>
<td colspan="2" rowspan="2"></td>
<td colspan="2" style="background-color:#1b85b8; border: 2px solid">Actual</td>			
</tr>
<tr>
<td>Non-grad</td>
<td>Graduate</td>
</tr>
<tr>
<td rowspan="2" style="background-color:#ae5a41; border: 2px solid">Predicted</td>
<td>Non-grad</td>
<td  style="background-color:#c3cb71; border: 2px solid"><b>a</b></td>
<td ><b>b</b></td>
</tr>
<tr>
<td>Graduate</td>
<td style="background-color:#c3cb71; border: 2px solid"><b>c</b></td>
<td><b>d</b></td>
</tr>
</table>

- Sensitivity (recall) = $\frac{a}{(a+c)}$
- Of all the **non-graduate** cases, what percentage do we correctly identify (recall)?
- Useful if we are interested in rare-event models where we want to accurately 
identify rare events, and are less worried about how accurate we are with the modal 
or common case. 

## Confusion Matrix: Specificity

<table>
<tr>
<td colspan="2" rowspan="2"></td>
<td colspan="2" style="background-color:#1b85b8; border: 2px solid">Actual</td>			
</tr>
<tr>
<td>Non-grad</td>
<td>Graduate</td>
</tr>
<tr>
<td rowspan="2" style="background-color:#ae5a41; border: 2px solid">Predicted</td>
<td>Non-grad</td>
<td><b>a</b></td>
<td style="background-color:#c3cb71; border: 2px solid"><b>b</b></td>
</tr>
<tr>
<td>Graduate</td>
<td><b>c</b></td>
<td style="background-color:#c3cb71; border: 2px solid"><b>d</b></td>
</tr>
</table>

- Specificity (positive predictive value) = $\frac{d}{(b+d)}$ or 
false alarm (1-specificity) = $\frac{b}{(b+d)}$
- Of all the **graduate** cases, what proportion actually do we predict correctly?
- This metric is useful as the balancing metric (false alarm) that we seek 
to hold constant while increasing our sensitivity. 


## Comparing and contrasting apples to apples

- There has been too much inconsistency in the application of accuracy metrics
to make comparisons useful (Gleason and Dynarski, 2002; Jerald, 2006;
Bowers et al., 2013). 
- Most of the 110 at-risk flags found in the  literature only include a measure 
of the sensitivity, or the specificity, but rarely both Bowers et al. (2013).
- In an effort to bring cohesion and clarity to the comparison of EWIs, Bowers
et al. (2013) calculated the performance metrics for 110 separate EWIs found in the 
literature


## Receiver Operating Characteristic

- ROC represents the tradeoff between the fraction of non-graduates identified out of 
all non-graduates, and the fraction of false non-graduates out of all graduates 
- Can support decision analysis by allowing a decision to be made explicitly about the 
balance between false-positives and false-negatives
- Excellent for optimizing rare-class identification


## What is out there? {.flexbox .vcenter}

<small>Adapted from Bowers, Sprott, and Taff 2013</small> 

```{r ewslitplot, fig.align='center', fig.height=7, fig.width=9.5}
library(png)
library(grid)
img <- readPNG("img/ewsLITplot2.png")
grid.raster(img, interpolate = TRUE)
```

## What we really want to know
- Limitation is that in **most** cases these are merely measures of how the model 
performed in-sample
- We actually want to *estimate* the likely accuracy on students we are making predictions 
for today
- This is critical to inform stakeholders about the appropriate weight to give 
the evidence provided by the DEWS score
- Estimating accuracy on future data is a key feature of machine learning

## Estimating out-of-sample error rates
- Unlike model fit statistics such as AIC, BIC, and $R^{2}$, out of sample 
fit statistics require re-estimation of the model
- Select a hold-out data set with observed outcome data and evaluate all models 
on their performance with that data
- Hard to do this locally, easier to do at state scale

```{r cvtable, fig.align='center', fig.height=3, fig.width=9.5}
library(png)
library(grid)
img <- readPNG("img/cvtable.png")
grid.raster(img, interpolate = TRUE)
```

## Why accuracy? 

**Opportunity cost**

>- Accuracy matters tremendously at scale. 
>- 1,000 schools receiving on average 240 predictions each. 
>- Each prediction reviewed by 3-5 staff for ~5 minutes
>- 3 x 240 x $\frac{1}{12}$ = 60 hours
>- 5 x 240 x $\frac{1}{12}$ = 100 hours
>- Across 1,000 schools thats 60,000 to 100,000 hours of work 
>- This could cost from $1 million to $4 million annually

## What does this mean in Wisconsin?

- Trying to take advantage of economies of scale -- one big accurate analysis 
disseminated statewide
- Unfortunately working statewide means we have a lot of observations, but a 
dearth of measures
- Context matters greatly and modeling strategies need to be able to reflect 
this context
- WI has a high graduation rate, so there is a needle in a haystack problem 



## The DEWS Workflow

```{r echo=FALSE, fig.align='center', fig.height=7, fig.width=9.5}
library(png)
library(grid)
img <- readPNG("img/DEWS_workflow_diagram.png")
grid.raster(img)
# <div style="width:300px; height:200px; align:center">
# ![DEWS Workflow](img/DEWS_workflow_diagram.png)
# </div>
```

## Wisconsin Variables

```{r predtable, echo=FALSE, fig.align='center', fig.width=8, fig.height=3.5}
library(png)
library(grid)
img <- readPNG("img/predictorTable.png")
grid.raster(img)
```


## A word on data preparation

- Defining who is a non-completer and who is a completer is difficult and essential 
to success
- Predictors should be transformed -- categories collapsed, numeric indicators 
centered and scaled, zero-variance data elements dropped
- Worth an entire paper to discuss challenges here (Kuhn 2013; James et al. 2012)

## Modeling

- In Wisconsin we are looking to make tens of thousands of bi-annual predictions and 
asking educators to pay attention to them
- Start with some more basic models, build complexity in order to increase 
accuracy (James et al. 2013)

```{r echo=FALSE, fig.height=4, fig.width=6, fig.align='center', message=FALSE, warning=FALSE}
library(eeptools)
x    <- c(1, 2, 3, 4, 5, 6, 7, 8)
y    <- c(14, 12, 10, 8, 6, 4, 2, 0)

jitter <- function(x) x + runif(1, min=-.5, max=.5)
x <- sapply(x, jitter)
y <- sapply(y, jitter)

labs <- c("Lasso", "Subset Selection", "Least Squares", 
          "Generalized Linear Models", "KNN", "Trees", "Bagging, Boosting", 
          "SVM")

qplot(x, y, geom='text', label=labs) + theme_classic() + 
  scale_x_continuous("Flexibility", limits=c(min(x) - 0.5, max(x) + 0.5)) +
  scale_y_continuous("Interpretability", limits=c(min(y) - 0.5, max(y) + 0.5)) +
  labs(title="Functional Forms and Tradeoffs") + theme_dpi(base_size=16) +
  theme(axis.text=element_blank(), axis.ticks=element_blank())
```

## Algorithm Search

- Start with 35 candidate algorithms
- Select 40,000 training observations and use 10 fold cross validation to tune 
the parameters for each algorithm
- Calculate the AUC on the test data for each algorithm
- Toss out models with abnormally long run times for practical reasons
- Select the top 5 to 8 models by AUC or by algorithmic diversity

## Ensemble

- Retrain the selected models with more data and a wider tuning parameter search
- Ensemble the models into a meta-model
- Calculate AUC on a validation data set (not the training or test set)
- Store meta-model for later scoring of new cases

## Logit/Probit Model Performance

```{r glmperf, echo=FALSE, fig.align='center'}
library(png)
library(grid)
img <- readPNG("img/ROCforGLM.png")
grid.raster(img)
```

## Caret Model Performance

```{r algoperf, echo=FALSE, fig.align='center'}
library(png)
library(grid)
img <- readPNG("img/caretROCs.png")
grid.raster(img)
```


## Ensemble Performance

```{r ensembleperf, echo=FALSE, fig.align='center'}
library(png)
library(grid)
img <- readPNG("img/ensembleROC.png")
grid.raster(img)
```

## Why ensemble?

- Not much added work beyond the above tasks
- Hedges against overfit and borrows strength from diverse algorithms 
(Kuncheva and Whitaker, 2003; Sollich and Krogh, 1996)
- Lots of room for improvement here including optimizing a more sophisticated 
meta-model beyond weighted average of individual predictions

## Test Data Performance

```{r testdatperf, echo=FALSE, fig.align='center'}
library(png)
library(grid)
img <- readPNG("img/testDataROCs.png")
grid.raster(img)
```

## Prediction

- The whole ballgame is to make predictions on current students at scale
- Need to assign them to a category (low, medium, high risk) based on predicted 
probabilities
- Calculate an optimal probability threshold by consulting with content experts 
and determining that between 10 and 25 false-positives would be acceptable 
for one less false-negative
- The later the intervention (and prediction grade) the less acceptable false-positives 
become

## Algorithms Chosen By Grade

```{r algotable, fig.align='center', fig.height=5, fig.width=8}
library(png)
library(grid)
img <- readPNG("img/algotable.png")
grid.raster(img, interpolate = TRUE)
```

## Combined Test and Training Data Results for Grade 7

Confusion Matrix  | Observed Grad | Observed Non. Grad
------------- | ------------- | -------------------
Pred. Grad.   | 84,744        | 3,670 
Pred. Non. Grad. | 13,718     | 7,454

## Challenges with Machine Learning 

- Hard to interpret results
- Difficult to get stability
- Concerns with the "black box"

## Overcoming those Challenges

- Communication, communication, communication
- Everyone agrees that accuracy is the priority, so the complexity is required
- Find ways to make complexity approachable, digestable, and trustable
- Transparency is key

## Implementation

- Not worth anything if predictions do not reach educators
- Use state reporting tool to disseminate results + a rollout plan
- Break score into risk bands of low, moderate, and high
- Construct subscores based on **malleable** student factors of academics, 
attendance, mobility, and behavior

## Student Profile

```{r stuprof, echo=FALSE, fig.align='center', fig.height=8, fig.width=5}
library(png)
library(grid)
img <- readPNG("img/studentprofile.png")
grid.raster(img)
```



## DEWS Box Zoom

```{r dewsbox=FALSE, fig.align='center', fig.height=6, fig.width=8}
library(png)
library(grid)
img <- readPNG("img/EWSPanel.png")
grid.raster(img)
```


## Peaking Under the Black Box

```{r varimp, echo=FALSE, fig.align='center', fig.height=8.2, fig.width=7.8}
library(png)
library(grid)
img <- readPNG("img/VariableImportance.png")
grid.raster(img)
```



## Contact Info

- DEWS Homepage: [http://www.dpi.wi.gov/dews](http://www.dpi.wi.gov/dews)
- E-mail: jknowles@gmail.com
- EWS Software on GitHub: [https://github.com/jknowles/ModelEWS](https://github.com/jknowles/ModelEWS)
- GitHub: [http://www.github.com/jknowles](http://www.github.com/jknowles)
- Homepage: [www.jaredknowles.com](http://www.jaredknowles.com/)
- Twitter: @jknowles
